{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from mne.decoding import (\n",
    "    CSP,\n",
    "    GeneralizingEstimator,\n",
    "    LinearModel,\n",
    "    Scaler,\n",
    "    SlidingEstimator,\n",
    "    Vectorizer,\n",
    "    cross_val_multiscore,\n",
    "    get_coef,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "subject_id = \"dvob\"\n",
    "fname = Path.cwd().parent.parent.parent / \"subjects\" / subject_id / \"EEG\" / \"regularity\" / \"raw_prep.fif\"\n",
    "raw = mne.io.read_raw_fif(fname, preload=True)\n",
    "\n",
    "\n",
    "## get events\n",
    "events_orig, events_dict = mne.events_from_annotations(raw)\n",
    "events = events_orig.copy()\n",
    "\n",
    "## remove new segment ans s140 (???) from events and update the events_dict\n",
    "for d_ev in [\"New Segment/\", \"Stimulus/S140\"]:\n",
    "    if d_ev in events_dict:\n",
    "        ns_id = events_dict[d_ev] \n",
    "        events = events[events[:, -1] != ns_id] \n",
    "        events_dict.pop(d_ev)\n",
    "\n",
    "trigger_dict = {\n",
    "                \"f1_std_or\": 1, \"f2_std_or\": 2, \"f3_std_or\": 3, \"f4_std_or\": 4,\n",
    "                \"f1_std_rndm\": 5, \"f2_std_rndm\": 6, \"f3_std_rndm\": 7, \"f4_std_rndm\": 8,\n",
    "                \"f1_tin_or\": 11, \"f2_tin_or\": 12, \"f3_tin_or\": 13, \"f4_tin_or\": 14,\n",
    "                \"f1_tin_rndm\": 15, \"f2_tin_rndm\": 16, \"f3_tin_rndm\": 17, \"f4_tin_rndm\": 18\n",
    "                } # copied from trigger definition script\n",
    "\n",
    "events_dict_new = {}\n",
    "for key, val in events_dict.items():\n",
    "    for trig_id, trig_val in trigger_dict.items():\n",
    "        if key.endswith(f\" {trig_val}\"):\n",
    "            events_dict_new[trig_id] = val\n",
    "\n",
    "diff_thr = 750\n",
    "split_indices = np.where(np.diff(events[:, 0]) > diff_thr)[0] + 1\n",
    "blocks = np.split(events, split_indices)\n",
    "\n",
    "## some checks\n",
    "assert len(blocks) == 12, f\"Something fishy with blocking, got {len(blocks)} blocks instead of 12\"\n",
    "for block_idx, block in enumerate(blocks):\n",
    "    assert len(block) == 500, f\"Number of triggers in block {block_idx + 1} is {len(block)}, must be 500.\" \n",
    "\n",
    "## concatenating similar blocks\n",
    "std_ord_block_idxs = [0, 2, 4]\n",
    "std_rnd_block_idxs = [1, 3, 5]\n",
    "tin_ord_block_idxs = [6, 8, 10]\n",
    "tin_rnd_block_idxs = [7, 9, 11]\n",
    "\n",
    "first_event = [0, 0, 99999] \n",
    "blocks_dict = {}\n",
    "for block_idxs, title in zip([std_ord_block_idxs, std_rnd_block_idxs, \\\n",
    "                                tin_ord_block_idxs, tin_rnd_block_idxs], \\\n",
    "                                [\"std_ord\", \"std_rnd\", \"tin_ord\", \"tin_rnd\"]):  \n",
    "    \n",
    "    blocks_dict[title] = np.concatenate(\n",
    "                                        [blocks[i] for i in block_idxs]\n",
    "                                        )\n",
    "    # blocks_dict[title] = np.insert(blocks_dict[title], 0, first_event, axis=0)\n",
    "    \n",
    "\n",
    "## binary classification for 2 groups\n",
    "\n",
    "\n",
    "\n",
    "## entropy level decoding (nothing removed)\n",
    "blocks_dict[\"std_ord\"][:, -1] = 101\n",
    "blocks_dict[\"std_rnd\"][:, -1] = 102\n",
    "blocks_dict[\"tin_ord\"][:, -1] = 103\n",
    "blocks_dict[\"tin_rnd\"][:, -1] = 104\n",
    "events_decode_1 = np.concatenate(list(blocks_dict.values()))\n",
    "\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "                    raw,\n",
    "                    events_decode_1,\n",
    "                    tmin=-0.3,\n",
    "                    tmax=0.7,\n",
    "                    preload=True,\n",
    "                    baseline=(None, 0)\n",
    "                    )\n",
    "\n",
    "epochs.pick(picks=\"eeg\")\n",
    "X = epochs.get_data()\n",
    "y = epochs.events[:, 2]\n",
    "\n",
    "clf = make_pipeline(\n",
    "                    StandardScaler(),\n",
    "                    LinearDiscriminantAnalysis(solver=\"svd\"), # multi class\n",
    "                    )\n",
    "time_gen = GeneralizingEstimator(clf, scoring=\"accuracy\") # good score for multiple class\n",
    "time_gen.fit(X, y)\n",
    "scores_cv = cross_val_multiscore(time_gen, X, y, cv=2, n_jobs=1)\n",
    "scores = np.mean(scores_cv, axis=0) # fix scores shape and save them\n",
    "\n",
    "## extract spatial patterns\n",
    "\n",
    "## 4 targets std_or, std_rnd, tin_or, tin_rnd\n",
    "## so here all epochs X, y (not important which frequency they are, finally y should be shape (,4))\n",
    "\n",
    "\n",
    "## sound-to-sound decoding\n",
    "## 4 targets, no matter which block... (y should be again (,4))\n",
    "\n",
    "\n",
    "## get coef for source\n",
    "\n",
    "## 4 targets, f1, f2, f3, f4 of standard -> clf_std trained on rnd ones (blocks 1, 3, 5)\n",
    "## 4 targets, f1, f2, f3, f4 of tinnitus -> clf_tin trained on rnd ones (blocks 7, 9, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_id = 8\n",
    "\n",
    "indices_of_carrier = np.where(blocks[1][:, -1] == ev_id)[0]\n",
    "selected_events = blocks[1][indices_of_carrier - 1]\n",
    "unique, counts = np.unique(selected_events[:, 2], return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "counts_dict = dict(zip(unique, counts))\n",
    "min_count = min(counts_dict.values()) # the minimum count to balance\n",
    "\n",
    "# Randomly downsample each trial type to the minimum count\n",
    "balanced_indices = []\n",
    "for trial_id in unique:\n",
    "    trial_indices = np.where(selected_events == ev_id)[0]\n",
    "    selected = np.random.choice(trial_indices, min_count, replace=False)\n",
    "    balanced_indices.extend(indices_of_carrier[selected - 1])\n",
    "\n",
    "len(blocks[1][balanced_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
